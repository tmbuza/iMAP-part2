---
title: "Systematic Microbiome Data Analysis in R"
subtitle: "End to End Practical User Guides"
author: "Teresia Mrema-Buza, A Microbiome Data Science Enthusiast and Owner of the Complex Data Insights, LLC, USA"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
knit: bookdown::render_book
output:
  bookdown::bs4_book:
    includes:
      in_header:
        
  bookdown::gitbook:
    includes:
      in_header: header.html
    config: 
      toc:
       collapse: 
      search: yes
      fontsettings:
        size: 2
    split_by: section
    split_bib: no
    df_print: paged
    number_sections: true
    css:
    - css/style.css
    - style.css
    
  html_document:
    code_folding: hide
    
  bookdown::html_document2:
    code_folding: hide
    
  bookdown::pdf_book:
    config:
      toc = true
      toc_depth = 2
      number_sections = true
      fig_caption = true
      keep_tex = no
      pandoc_args = null
      toc_unnumbered = no
      toc_appendix = no
      toc_bib = no 
      quote_footer = null
      highlight_bw = true
      latex_engine = xelatex
      df_print = kable
      base_format = rmarkdown::pdf_document
  includes:
    in_header: 
    - latex/header.tex
    - latex/preamble.tex
documentclass: book
classoption: openany #remove empty pages in pdf doc
bibliography:
- library/book.bib
- library/packages.bib
- library/microbiome.bib
- library/software.bib 
citation_package:
- natbib
- biblatex
- amsplain
colorlinks: true
css:
- css/style.css
always_allow_html: true
fig_caption: true
fontsize: 12pt
geometry: margin=1in
indent: false
keep_tex: true
link-citations: true
mainfont: Times New Roman
biblio-style: apalike
spacing: double
header-includes: 
- \usepackage{setspace}
- \newpage
- \newenvironment{tmbinfo}[0]{}{}
- \renewenvironment{tmbinfo}[0]{}{}
- \newenvironment{tmbalert}[0]{}{}
- \renewenvironment{tmbalert}[0]{}{}
- \newenvironment{tmbshare}[0]{}{}
- \renewenvironment{tmbshare}[0]{}{}
description: |
  This is a practical user's guide for **Systematic Microbiome Data Analysis in R**. The guide provides integrated and highly curated solutions for achieving better results.
---


```{r pkgbiblib, include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'base','bookdown','rmarkdown','tidyverse','shiny','vegan','data.table, dendextend, robCompositions, microbiome, ALDEx2, caret, rms, phyloseq'
), 'library/packages.bib')
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

## Create Object Folder 
if (!dir.exists("RDataRDS")) {dir.create("RDataRDS")}
if (!dir.exists("GIFS")) {dir.create("GIFS")}

source(file = "R/common.R")

htmltools::tagList(rmarkdown::html_dependency_font_awesome())

knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  comment = NA,
  fig.path='./Figures/',
  fig.show='asis',
  dev = 'png',
  fig.align='center',
  out.width = "80%",
  out.height = "40%",
  fig.width = 7,
  fig.asp = 0.618,
  fig.show = "asis"
)
```

# Bioinformatics Analysis of  Microbiome Data {-#frontpage}

<br>

<h2>Quick Glimpse</h2>

Microbiome data analysis is about asking questions to understand the microbial composition in a given sample. The Systematic Microbiome Data Analytics (**SMDA**) book series <a href=""><img src="images/cover2.png" alt="Book cover" width="50%" style="padding: 0 15px; float: right;"/></a> represent practical guides supporting the microbiome data analytics beyond the traditional analysis. This on-going project supports the transformation of complex microbiome data into optimal biological insights. For simplicity, the eBook is divided in four parts (Part 21 through Part 4). These series provide users with integrated solutions for gaining insight into microbial community biodiversity and investigating their role in disease, health and their impact in the environment.

<br>

<h2>License</h2>

 ![](images/CCbyNCND.png){ width=100px } The online version of this book is free and licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. The code that built this guide is available at a private GitHub repository. A minimal code and quick description of the book is available at a public [GitHub Page](https://tmbuza.github.io/Systematic-Microbiome-Data-Analysis/). If interested you can obtain a more detailed copy from the [CDI LLC Store](https://complexdatainsights.com/products).

<!--chapter:end:index.Rmd-->

# (PART) BIOINFORMATICS ANALYSIS {-}

# Preprocessing 16S rRNA Sequencing Data {#preprocessing-16S-reads}

## Initial read quality scores
We will use fastqc software to explore the original read qualities before trimming and decontamination.

```bash
mkdir data/fastqc1
fastqc *.fastq.gz -o data/fastqc1
```

```bash
mkdir data/multiqc1
multiqc -f --data-dir data/fastqc1 -o data/multiqc1 --export
```

> Plots from `multiqc` are exported to `multiqc_plots` folder. 

![Original: Mean quality scores](data/multiqc1/multiqc_plots/png/mqc_fastqc_per_base_sequence_quality_plot_1.png){width=100%}

<br>

![Original: Per sequence quality scores](data/multiqc1/multiqc_plots/png/mqc_fastqc_per_sequence_quality_scores_plot_1.png){width=100%}

<br>

## Read trimming using `bbduk.sh` from bbmap platform
> Hint! Make sure that the pattern in the script is in the file name. Some files may contain `R1_001.fastq.gz`. Here we use files downloaded from NCBI-SRA, they do not use R1_001 in the sample name.

```bash
for i in `ls -1 *_1.fastq.gz | sed 's/_1.fastq.gz//'`
  do
  bbduk.sh -Xmx4g in1=$i\_1.fastq.gz in2=$i\_2.fastq.gz out1=data/trimmed/$i\_1.fastq.gz out2=data/trimmed/$i\_2.fastq.gz qtrim=r trimq=25 overwrite=True
  done
```

```bash
mkdir -p data/stats2  
seqkit stat data/trimmed/*.fastq.gz >data/stats2/seqkit_stats.txt

mkdir data/fastqc2
fastqc data/trimmed/*.fastq.gz -o data/fastqc2

mkdir data/multiqc2
multiqc -f --data-dir data/fastqc2 -o data/multiqc2 --export
```

![Trimmed: Mean quality scores](data/multiqc2/multiqc_plots/png/mqc_fastqc_per_base_sequence_quality_plot_1.png){width=100%}

<br>

![Trimmed: Per sequence quality scores](data/multiqc2/multiqc_plots/png/mqc_fastqc_per_sequence_quality_scores_plot_1.png){width=100%}

## Read decontamination
- Using `bbduk.sh` from bbmap platform
- This remove some contamination, e.g. PhiX Control reads.

```bash
for i in `ls -1 *_1.fastq.gz | sed 's/_1.fastq.gz//'`
do
bbduk.sh -Xmx4g in1=data/trimmed/$i\_1.fastq.gz in2=data/trimmed/$i\_2.fastq.gz out1=data/decontam/$i\_1.fastq.gz out2=data/decontam/$i\_2.fastq.gz outm1=data/decontam/matchedphix/$i\_1.fastq.gz outm2=data/decontam/matchedphix/$i\_2.fastq.gz ref=~/bbmap/resources/phix174_ill.ref.fa.gz k=31 hdist=1 overwrite=True
done
```

```bash
mkdir -p data/stats3  
seqkit stat data/decontam/*.fastq.gz >data/stats3/seqkit_stats.txt

mkdir data/fastqc3
fastqc data/decontam/*.fastq.gz -o data/fastqc3

mkdir data/multiqc3
multiqc -f --data-dir data/fastqc3 -o data/multiqc3 --export
```

<br>

![Decontaminated: Mean quality scores](data/multiqc3/multiqc_plots/png/mqc_fastqc_per_base_sequence_quality_plot_1.png){width=100%}

<br>

![Decontaminated: Per sequence quality scores](data/multiqc3/multiqc_plots/png/mqc_fastqc_per_sequence_quality_scores_plot_1.png){width=100%}

<br>

## Compare preprocessed reads
```{r}
library(tidyverse, suppressPackageStartupMessages())
library(ggtext)

stats1 <- read_table("data/stats1/seqkit_stats.txt") %>% 
  mutate(file = str_replace_all(file, ".*/", "")) %>% 
  select(file, original = num_seqs)
saveRDS(stats1, "RDataRDS/seqkit_stats.rds")

stats2 <- read_table("data/stats2/seqkit_stats.txt") %>% 
  mutate(file = str_replace_all(file, ".*/", "")) %>% 
  select(file, trimmed = num_seqs)

stats3 <- read_table("data/stats3/seqkit_stats.txt") %>% 
  mutate(file = str_replace_all(file, ".*/", "")) %>% 
  select(file, decontaminated = num_seqs)

read_status <- inner_join(stats1, stats2, by = "file") %>% 
  inner_join(., stats3, by = "file") %>% 
  mutate(strand = ifelse(str_detect(file, "_1"), "foward", "reverse"), .before=original) %>%
  pivot_longer(cols = -c(file, strand), names_to = "variable", values_to = "num_seqs")

head(read_status)
```

## What is the distribution of the processed reads

```{r}
read_status %>% 
  mutate(variable = factor(variable),
         variable = fct_reorder(variable, num_seqs, .desc=TRUE)) %>% 
  ggplot(aes(x = strand, y = num_seqs/1000, fill = variable)) +
  geom_col(position = "dodge") +
  labs(x = NULL, y = "Number of Reads (1000)", fill = "Preprocess") +
  theme_classic() +
  theme(axis.text.x = element_markdown(),
        legend.text = element_text(face = NULL),
        legend.key.size = unit(12, "pt")) + nowhitespace
```


<br>

# Preprocessing Metagenomics Sequencing Data {#preprocess-metag-reads}

## What are the read statistics 
- Using `seqkit stat` function.
- Run the function on command line.

```bash
mkdir -p data
mkdir -p data/stats1  
seqkit stat *.fastq.gz >data/stats1/seqkit_stats.txt

```

## Initial read quality scores
The `kneaddata` from biobakery platform is exclusively used to preprocess the metagenomics data.
- Here we set the `kneaddata` to run `fastqc` as the first step using `run-fastqc-start` function.
- Then `fastqc` results are then summarized outside `kneaddata` pipeline using `multiqc` software.

```bash
for i in data/*.fastq.gz
do
time kneaddata --i $i \
-o kneaddata_fastqc1 \
--run-fastqc-start \
--bypass-trim \
--bypass-trf \
--sequencer-source "none" \
-t 4 
done


mkdir -p kneaddata_fastqc1/multiqc1
for i in kneaddata_fastqc1/fastqc
    do  
        multiqc -f --data-dir $i -o kneaddata_fastqc1/multiqc1 --export
    done

```

## Read Trimming
- Using trimmomatic tool to trim the reads using the specified options.
- The fastqc function is run after trimming
- Then fastqc results are summarized using multiqc software.

```bash

for i in data/*.fastq.gz
do
  time kneaddata --i $i \
    -o kneaddata_FastQC2 \
    --trimmomatic /Users/tmbuza/opt/anaconda3/envs/biobakery3/bin/ \
    --trimmomatic-options \
        "ILLUMINACLIP:/Volumes/SeagateTMB/trimmomatic-0.36/adapters/NexteraPE-PE.fa:2:30:10 \
        LEADING:3 \
        TRAILING:3 \
        SLIDINGWINDOW:4:20 \
        MINLEN:60" \
    --sequencer-source "NexteraPE" \
    --run-fastqc-end \
    --bypass-trf \
    -t 4 
done


seqkit stat kneaddata_fastqc2/*trimmed.fastq >QC/QC2_trimmed_stats.txt
mkdir -p kneaddata_fastqc2/multiqc12
for i in kneaddata_fastqc2/fastqc
    do  
        multiqc -f --data-dir $i -o kneaddata_fastqc2/multiqc12 --export
    done

```


## Read Decontamination
### Option 1: Using Bowtie2 database
- Trimming is done using trimmomatic
- Then decontamination is done using the bowtie2 database.
- Then after decontamination fastqc is implemented to assess the read quality scores of the remaining reads.
- Then fastqc results are summarized using multiqc software.

```bash
for i in data/*.fastq.gz
do
time kneaddata --i $i \
  -o kneaddata_fastqc3 \
      --reference-db /Volumes/SeagateTMB/kneaddata_database/ \
      --trimmomatic /Users/tmbuza/opt/anaconda3/envs/biobakery3/bin/ \
      --trimmomatic-options \
          "ILLUMINACLIP:/Volumes/SeagateTMB/trimmomatic-0.36/adapters/NexteraPE-PE.fa:2:30:10 \
          LEADING:3 \
          TRAILING:3 \
          SLIDINGWINDOW:4:20 \
          MINLEN:60" \
      --sequencer-source "NexteraPE" \
      --run-trf \
      --run-fastqc-end \
      -t 4 
done

seqkit stat kneaddata_fastqc3/*kneaddata.fastq >QC/QC3_bowtie2decont_stats.txt
mkdir -p kneaddata_fastqc3/multiqc13
for i in kneaddata_fastqc3/fastqc
    do  
        multiqc -f --data-dir $i -o kneaddata_fastqc3/multiqc13 --export
    done

```

### Option 2: Using BMTagger database 
- Trimming is done using trimmomatic
- Then decontamination is done using the BMTagger database.
- Then after decontamination fastqc is implemented to assess the read quality scores of the remaining reads.
- Then fastqc results are summarized using multiqc software.

```bash
for i in data/*.fastq.gz
do
time kneaddata --i $i \
  -o kneaddata_fastqc4 \
      --reference-db /Volumes/SeagateTMB/Human_Assembly19_BMTagger_DB/ \
      --trimmomatic /Users/tmbuza/opt/anaconda3/envs/biobakery3/bin/ \
      --trimmomatic-options \
          "ILLUMINACLIP:/Volumes/SeagateTMB/trimmomatic-0.36/adapters/NexteraPE-PE.fa:2:30:10 \
          LEADING:3 \
          TRAILING:3 \
          SLIDINGWINDOW:4:20 \
          MINLEN:60" \
      --sequencer-source "NexteraPE" \
      --run-trf \
      --run-bmtagger \
      --run-fastqc-end \
      -t 4 
done

seqkit stat kneaddata_fastqc4/*kneaddata.fastq >QC/QC4_bmtaggerdecont_stats.txt
mkdir -p kneaddata_fastqc4/multiqc14
for i in kneaddata_fastqc4/fastqc
    do  
        multiqc -f --data-dir $i -o kneaddata_fastqc4/multiqc14 --export
    done

```


<!--chapter:end:02a_read_preprocessing.Rmd-->

# Taxonomic Profiling of Microbiome Data

## Generalized bioinformatics workflow
```{r taxonomic_profiling_output, echo=FALSE}
library(DiagrammeR)
library(DiagrammeRsvg)

DiagrammeR::grViz("
digraph {
graph [layout = dot, rankdir = TD]

node [
shape = box, 
style = filled, 
fillcolor = white, 
fontname = Helvetica,
penwidth = 2.0] 

edge [arrowhead = diamond]

A [label = 'TAXONOMIC PROFILING OF MICROBIOME DATA', fillcolor = white, penwidth = 5.0]
B [label = 'Complex Data from Sequencing Platforms\n(*.fastq.gz)', shape = folder]
C [label = '16S rRNA Gene Amplicon\nSequencing Data']
D [label = 'Metagenomics Shotgun\nSequencing Data']
E [label = 'Mothur\nPipeline', shape = oval]
F [label = 'QIIME2\nPipeline', shape = oval]
G [label = 'MetaPhlAn\nPipeline', shape = oval]
H [label = 'HUMAnN\nPipeline', shape = oval]
I [label = 'ASV/OTU &\nTaxonomy Tables']
J [label = 'Feature & Taxonomy\nTables']
K [label = 'Microbial\nProfiles']
L [label = 'MetaPhlAn\nBugs List']
M [label = 'Tidying Mothur, QIIME2,\nMetaPhlAn & HUMAnN Output', shape = oval, penwidth = 2.0]
N [label = 'TIDY MICROBIAL\nABUNDANCE\nTABLES', shape = diamond, fillcolor = yellow, penwidth = 4.0]

A [color = gray]
C,E,F,I,J [color = limegreen]
D,G,H,K,L [color = dodgerblue]

{A}  -> B
{B}  -> C
{B}  -> D
{C}  -> E
{C}  -> F
{D}  -> G
{D}  -> H
{E}  -> I
{F}  -> J
{G}  -> K
{H}  -> L
{I}  -> M [arrowhead = vee]
{J}  -> M [arrowhead = vee]
{K}  -> M [arrowhead = vee]
{L}  -> M [arrowhead = vee]
{M}  -> N 


}", height = 500, width = 500)

# O [label = 'Exploratory Analysis of\nMicrobial Abundance', shape = ellipse, fillcolor = lightgrey, penwidth = 0]
# {M}  -> N[label = ' R\n  Python']
# {N}  -> O [label = '  Next Step!\nMore\nBiological\nInsights']
```

<br>

## Format of input data
The default format used in this practical user guide is compressed fastq, `*.fastq.gz`.

```{block, type="tmbalert", echo=TRUE}
## Compressing fastq files using `gzip`
> Run the command below only if the sequencing data files are not compressed!

gzip *.fastq
```

<br>

# Mcrobial profiling using `mothur` pipeline.

## Download a Mothur trained classifer
- For demo purposes we will use [Silva seed](https://mothur.org/wiki/Silva_reference_files) due to its smaller size.
- Other classifiers can be found [here](https://mothur.org/wiki/taxonomy_outline/).

```bash
wget https://mothur.s3.us-east-2.amazonaws.com/wiki/silva.seed_v138_1.tgz
tar xvzf silva.seed_v138_1.tgz
rm *.tgz
```

## Mothur basic wrapper
- Below is a minimal wrapper for sequence processing, classification and taxonomic assignment using `mothur` pipeline.
- For more information refer to `mothur` [MiSeq SOP](https://mothur.org/wiki/miseq_sop/).
- We can save this wrapper as `mothur_code.batch` and place it in a folder named code.
- Then we will run the file on command line as shown below. See the commented lines.
- We assume that the input data is gunzipped i.e. **.fastq.gz**. If not run `gzip *.fastq` to compress the files.

```{block, type="tmbalert", echo=TRUE}
1. Note that the [`shebang line`](https://en.wikipedia.org/wiki/Shebang_(Unix)) remind us that we are running the analysis on `mothur` command line NOT `bash`. This takes the advantage of using the **current** option to refer to the last saved file.
2. If not generating the mapping file automatically you should make sure that it confer to accepted format as in `mothur_mapping_files.tsv` described in the previous section. In this demo we use `bush.files`. This means that all output files will be prefixed with the term <u>bush</u> to reflect the name of the project. Try to avoid loner names.
```


```bash
#!/mothur

# Usage: mothur code/mothur_code.batch # if mothur is in the path
# Usage: ./mothur code/mothur_code.batch # on mac/linux if the executable `mothur` is in the working directory.
# Usage: ./mothur.exe code/mothur_code.batch # on Windows machins if the executable `mothur.exe` is in the working directory.

set.current(processors=1)
# set.logfile(name=make_files.logfile)
# make.file(inputdir=., type=fastq.gz, prefix=test)

set.logfile(name=seq_assembly.logfile)
make.contigs(file=bush.files, outputdir=./test, maxambig=0, maxlength=275);
unique.seqs(count=current);
summary.seqs(fasta=current, count=current)

set.logfile(name=seq_align_preclustering.logfile)
align.seqs(fasta=current, reference=silva.seed_v138_1.align);
screen.seqs(fasta=current, count=current, start=13862, end=23444, maxhomop=8);
filter.seqs(fasta=current, vertical=T, trump=.);
pre.cluster(fasta=current, count=current, diffs=2);
unique.seqs(fasta=current, count=current);

set.logfile(name=chimera_removal.logfile)
chimera.vsearch(fasta=current, count=current, dereplicate=t);

set.logfile(name=silva_seed_classification.logfile)
classify.seqs(fasta=current, count=current, reference=silva.seed_v138_1.align, taxonomy=silva.seed_v138_1.tax, cutoff=100);
remove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota);

set.logfile(name=final_files.logfile)
rename.file(fasta=current, count=current, taxonomy=current, prefix=final)

set.logfile(name=otu_clustering.logfile)
dist.seqs(fasta=current, cutoff=0.03);
cluster(column=current, count=current, cutoff=0.03);
make.shared(list=current, count=current, label=0.03);
classify.otu(list=current, count=current, taxonomy=current, label=0.03);
make.lefse(shared=current, constaxonomy=current);
make.biom(shared=current, constaxonomy=current);

set.logfile(name=phylotype_clustering.logfile)
phylotype(taxonomy=current);
make.shared(list=current, count=current, label=1);
classify.otu(list=current, count=current, taxonomy=current, label=1);
make.lefse(shared=current, constaxonomy=current);
make.biom(shared=current, constaxonomy=current);

set.logfile(name=asv_clustering.logfile)
make.shared(count=current)
classify.otu(list=current, count=current, taxonomy=current, label=ASV)
make.lefse(shared=current, constaxonomy=current)
make.biom(shared=current, constaxonomy=current)

set.logfile(name=phylogenetic_clustering.logfile)
dist.seqs(fasta=current, output=lt)
clearcut(phylip=current)
```

<br>

# Mcrobial profiling using `qiime2` pipeline

### Download a QIIME2 trained classifer
- You can use Naive Bayes (nb) classifiers trained on GreenGenes or SILVA database with 99% OTUs. You can train your own classifier using the [q2-feature-classifier](https://github.com/qiime2/q2-feature-classifier).

Here we will download the smallest classifier, the naive classifier trained on [Greengenes 13_8 99% OTUs from 515F/806R region of sequences](https://docs.qiime2.org/2022.2/data-resources/)

```bash
wget \
  -O "gg-13-8-99-515-806-nb-classifier.qza" \
  "https://data.qiime2.org/2022.2/common/gg-13-8-99-515-806-nb-classifier.qza"
```

### QIIME2 installation
- We assumes that you have already installed QIIME 2. If not please do so using the instructions described [here!](https://docs.qiime2.org/2022.2/install/).

A simple demo for installing qiime2 on Mac OS
```bash
wget https://data.qiime2.org/distro/core/qiime2-2022.2-py38-osx-conda.yml
conda env create -n qiime2-2022.2 --file qiime2-2022.2-py38-osx-conda.yml
rm qiime2-2022.2-py39-osx-conda.yml

# Activate qiime environment 
conda activate qiime2-2022.2

# Confirm installation
qiime info
```

### Running QIIME2 commands
- We prefer to use the command line interface [g2cli](https://docs.qiime2.org/2022.2/interfaces/q2cli/).
- Activate the qiime2 environment: We are using qiime2-2022.2.

```bash
conda activate qiime2-2022.2
```



### Import paired-end fastq files
```bash
qiime tools import \
  --type 'SampleData[PairedEndSequencesWithQuality]' \
  --input-path pe-33-manifest.tsv \
  --output-path qiime2_bushmeat/paired-end-demux.qza \
  --input-format PairedEndFastqManifestPhred33V2
```


- Below is a minimal wrapper for
  - exploring the metadata,
  - importing the data,
  - sequence quality control,
  - sequence classification,
  - taxonomic assignment,
  - generating feature table.
  
For more information refer to `qiime2` [tutorials](https://docs.qiime2.org/2022.2/tutorials/).

```bash
#!/qiime

# Usage: qiime2 code/qiime2_code.bash
```

## QIIME2 output tidying
```{r}
taxlevels <- c("kingdom", "phylum", "class", "order", "family", "genus", "species")

read_delim("/Volumes/RedSeagate/MOTHUR_BUSHMEAT/SRR7450/qiime2_bushmeat/q2-transformed-tables/taxonomy.tsv") %>%
  rename_all(tolower) %>%
  rename(otu = "feature id") %>% 
  mutate(confidence = round(confidence, digits = 2)) %>%
  # filter(confidence == 1.00) %>% 
  mutate(taxon = str_replace_all(taxon, "; s__$", ""),
         taxon = str_replace_all(taxon, "; g__$", ""),
         taxon = str_replace_all(taxon, "; f__$", ""),
         taxon = str_replace_all(taxon, "; o__$", ""),
         taxon = str_replace_all(taxon, "; c__$", ""),
         taxon = str_replace_all(taxon, "; p__$", ""),
         taxon = str_replace_all(taxon, "; k__$", ""),
         ) %>% 
  separate(col = taxon, into = all_of(taxlevels), sep = "; ") %>%
  saveRDS("RDataRDS/q2_taxonomy.rds")
```

```{r}
read_delim("/Volumes/RedSeagate/MOTHUR_BUSHMEAT/SRR7450/qiime2_bushmeat/q2-transformed-tables/feature-table.tsv", skip = 1) %>%
  rename(otu = "#OTU ID") %>% 
  saveRDS("RDataRDS/q2_otutable.rds")

```

```{r}
readRDS("RDataRDS/q2_taxonomy.rds") %>% 
  inner_join(., readRDS("RDataRDS/q2_otutable.rds"), by="otu")
```

<br>
<br>

# Taxonomic Profiling of Metagenomics Sequencing Data
## Using MetaPhlAn pipeline.
- We loop through the fastq files using the following parameters.
- You can optionally customize these parameters to suit your needs.
- Note that most parameters are default setting except the *-t rel_ab_w_read_stats* which will add the read count to the output.

```bash

for i in data/*.fastq.gz
	do 
		metaphlan $i \
		--input_type fastq \
		--force \
		--bowtie2db /Volumes/SeagateTMB/metaphlan_databases/ \
		--nproc 4 \
		--stat_q 0.02 \
		--min_mapq_val 5 \
		--output_file ${i%}_metaphlan3_profile.txt \
		--bowtie2out ${i%}_metaphlan3_bowtie2out.txt \
		--biom ${i%}_metaphlan3_abundance.biom \
    -t rel_ab_w_read_stats
	done

```

## Output of taxonomic profiling
- OTU table in mothur and QIIME2
- MetaPhlAn pipeline output taxonomic profiles for each input fastq file. 
- You can further use some integrated functions to manage the output. For example, the command below merge relative abundance into a single file similar to a transposed OTU table from Mothur and QIIME platforms.


<!--chapter:end:02b_bioinfomatics_analysis.Rmd-->

# (APPENDIX) APPENDIX {-}

# Saved Data Objects

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## RDS Format for Individual Object
```{r}

rds <- list.files(path="./RDataRDS", pattern = "*.rds", full.names = TRUE)
rds

```

## RData Format for Multiple Objects
```{r}

rdata <- list.files(path="./RDataRDS", pattern = "*.RData", full.names = TRUE)
rdata

```


## CSV Format for Selected Dataframe
```{r}

csv <- list.files(path="./RDataRDS", pattern = "*.csv", full.names = TRUE)
csv

```

<!-- ## All-in-One Input-Output Data -->
<!-- ```{r} -->
<!-- lsdata("RDataRDS/saved_objects.RData") -->
<!-- ``` -->

## How to reload RDS or RData into R environment
```{block, type="tmbarrowF", echo=TRUE}

- RDS format e.g. foo.rds
  - foo <- readRDS("RDataRDS/foo.rds")

- RData format e.g. foo.RData
  - load("RDataRDS/foo.RData", verbose = TRUE)

- List objects in RData
  - lsdata("foo.RData") 

```


# Software and Packages Used

## Basic dependencies
* `r R.version.string`
* `tidyverse` (v. `r packageVersion("tidyverse")`)
* `bookdown` (v. `r packageVersion("bookdown")`)
* `ggpubr` (v. `r packageVersion("ggpubr")`)
* `downlit` (v. `r packageVersion("downlit")`)
* `phyloseq` (v. `r packageVersion("phyloseq")`)
* `ape` (v. `r packageVersion("ape")`)
* `ggtext` (v. `r packageVersion("ggtext")`)
* `dendextend` (v. `r packageVersion("dendextend")`)
* `metagMisc` (v. `r packageVersion("metagMisc")`)
* `cgwtools` (v. `r packageVersion("cgwtools")`)

## Available on machine used 
```{r}
sessionInfo()

```




<!--chapter:end:99_appendix.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`


<!--chapter:end:999-references.Rmd-->

